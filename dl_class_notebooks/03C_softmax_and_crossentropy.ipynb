{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$logits => \\operatorname{softmax}(logits) => \\operatorname{crossentropy}(labels, softmax)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "\n",
    "The softmax function takes an input vector and returns a probability distribution over the elements of the vector, such that the sum of the probabilities is 1.\n",
    "\n",
    "$\\operatorname{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$\n",
    "\n",
    "In this definition, `x_i` is the `i`-th element of the input vector `x`, and `n` is the length of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example Python function that calculates the softmax of an input vector `x`.\n",
    "\n",
    "In this example, the `softmax()` function takes an input vector `x` as a NumPy array. It calculates the exponential of each element in the input vector, then calculates the sum of the exponential values. Finally, it calculates the softmax values by dividing each exponential value by the sum.\n",
    "\n",
    "Note that this implementation assumes that the input vector is a one-dimensional NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Calculate the softmax of an input vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Input vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Softmax of the input vector.\n",
    "    \"\"\"\n",
    "    # Calculate the exponential of each element in the input vector\n",
    "    exp_x = np.exp(x)\n",
    "\n",
    "    # Calculate the sum of the exponential values\n",
    "    sum_exp_x = np.sum(exp_x)\n",
    "\n",
    "    # Calculate the softmax values by dividing each exponential value by the sum\n",
    "    softmax_x = exp_x / sum_exp_x\n",
    "\n",
    "    return softmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6652409557748219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3)/np.sum(np.exp([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorflow` provides a function to calculate softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.09003057 0.24472848 0.66524094], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "print(tf.nn.softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "Cross-entropy is used as a loss function in ML and DL for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(p, q) = - \\sum_{i=1}^N p_{i} \\cdot \\log(\\hat{q_{i}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, $p$ is the true probability distribution, $q$ is the predicted probability distribution, $N$ is the number of classes, `p_{i}` is the true probability of the $i$-th class, and $\\hat{q_{i}}$ is the predicted probability of the $i$-th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025851"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 0, 1]]\n",
    "y_pred = [[0.1, 0.8, 0.1]]\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the cross-entropy loss between two input vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy.ndarray\n",
    "        True probability distribution.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted probability distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross-entropy loss between the two input vectors.\n",
    "    \"\"\"\n",
    "    # Calculate the cross-entropy loss\n",
    "    # The basis of the log function used here is e:\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.log.html\n",
    "    loss = -np.sum(y_true * np.log(y_pred)) \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(0*np.log(0.1) + 0*np.log(0.8) + 1*np.log(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931472"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 0, 1]]\n",
    "y_pred = [[0.2, 0.3, 0.5]]\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
